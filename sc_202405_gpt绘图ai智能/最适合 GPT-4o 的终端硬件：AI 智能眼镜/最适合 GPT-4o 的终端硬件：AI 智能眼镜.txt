


            
最适合 GPT-4o 的终端硬件：AI 智能眼镜
          




5月13日，OpenAI 发布了 GPT-4o（“o”代表“omni”）。GPT-4o 是 OpenAI 最新的旗舰型号，它接受文本、音频和图像的任意组合作为输入，并生成文本、音频和图像输出的任意组合。与之前的 GPT-4 相比，GPT-4o 大幅改善了语音交互的延迟，并提高了对图像和音频的理解能力。在 GPT-4o 之前，我们可以使用语音模式与 ChatGPT 交谈。其中，GPT-3.5 的平均延迟为 2.8 秒，GPT-4 的平均延迟为 5.4 秒。而使用 GPT-4o 进行语音交互，平均延迟仅为 0.32 秒。GPT-4o 之所以能大幅改善语音交互的延迟，是因为它是一个端到端多模态大模型，它所有的输入和输出都由同一个神经网络进行处理。过去使用的语音模式则是一个由三个独立模型组成的管道，首先语音转文本模型（Whisper）将输入音频转换为文本，接着 GPT-3.5 或 GPT-4 接收文本并输出文本，最后文本转语音模型（TTS）将该文本转换为音频输出。GPT-4o 是一个端到端多模态大模型OpenAI 为什么要发布 GPT-4o在以 Llama 3 为代表的开源模型的步步紧逼之下，市场此前认为 OpenAI 可能会尽快发布 GPT-4.5 或 GPT-5 来应对 Llama 3 的挑战。但 OpenAI 却在这个时间点，发布了侧重于语音交互和视觉理解的 GPT-4o，这让很多人感到困惑——为什么 OpenAI 发布的是 GPT-4o，而不是 GPT-4.5 或 GPT-5？更令人费解的是，OpenAI 宣布 GPT-4o 向所有免费用户开放。要知道此前只有 ChatGPT Plus 付费订阅用户才可以使用 GPT-4，而现在所有免费用户都可以使用比 GPT-4 更先进的 GPT-4o 模型，这种做法似乎会削弱 Plus 用户的付费意愿。事实上，要理解 OpenAI 为什么要发布 GPT-4o，我们可以将以下三点信息关联起来：（1）GPT-4o 侧重于语音交互和视觉理解；（2）GPT-4o 向所有用户免费开放；（3）苹果最近考虑在 OpenAI 和谷歌之间二选一，选择一家公司为新款 iPhone 提供模型支持，而 OpenAI 则选择在谷歌开发者大会（5月14日）前一天发布 GPT-4o。首先，与 GPT-4 相比，GPT-4o 最大的变化在于改善了语音交互的延迟，并提高了对图像和音频的理解能力。这两项能力（低延迟语音交互、更好的图像和音频理解能力）非常契合智能手机。例如，在 GPT-4o 的官方演示中，我们可以看到演示人员使用 iPhone 摄像头实时拍摄数学方程的解题过程，并通过语音交互让 GPT-4o 来帮助自己得到正确的答案。OpenAI GPT-4o 发布会GPT-4o 官方演示其次，GPT-4o 向所有用户免费开放，有助于更多智能手机用户体验 GPT-4o，这能让 OpenAI 在和苹果的谈判中，基于用户口碑获得更多的谈判优势。毕竟在苹果二选一的情况下，对于 OpenAI 来说，全球超过10亿的 iPhone 手机用户是它不能失去的客户群体。所以 OpenAI 在这个时间点发布 GPT-4o，可能更多是为了促成与苹果的长期合作，以便深度集成到未来的 iPhone 手机中。最适合 GPT-4o 的硬件是智能眼镜虽然 GPT-4o 非常契合智能手机，但我们在使用过程中仍然存在痛点。就像我们在 GPT-4o 官方演示中看到的那样，我们需要使用一只手拿着手机拍摄，以便让 GPT-4o 能够看到并理解视觉场景，这导致使用过程缺乏灵活性，并且增加了交互的复杂度。因此，更适合 GPT-4o 的终端硬件，应该是能够解放我们的双手，让我们在处理复杂任务时拥有最大程度的操作灵活性和便捷度。目前有两种硬件方案可供选择。第一种是智能眼镜，智能眼镜配备摄像头、麦克风和扬声器，支持语音交互和图像视频拍摄。第二种是带有摄像头的智能耳机，这是在我们日常佩戴的智能耳机上增配摄像头。这两种方案的主要差异在于，智能眼镜的摄像头在正面，智能耳机的摄像头在侧面。正面的摄像头比侧面的摄像头拥有更大的拍摄角度，也更符合我们的视觉习惯，因此智能眼镜会比带有摄像头的智能耳机更适合 GPT-4o。AI 智能眼镜已得到市场验证在已发布的产品中，最具有代表性的 AI 智能眼镜是 Meta 联合雷朋发布的第二代 Ray-Ban Meta 智能眼镜。Ray-Ban Meta 智能眼镜就在三周前（4月23日），Meta 在 Ray-Ban Meta 智能眼镜中正式上线了 Llama 3 多模态功能。Llama 3 多模态大模型是一个类似于 GPT-4o 的大模型，它可以配合支持语音交互和图像视频拍摄功能的智能眼镜，来辅助我们解决复杂任务，或者担任我们的随身 AI 助手。Ray-Ban Meta 智能眼镜 + Llama 3 多模态大模型目前第二代 Ray-Ban Meta 智能眼镜累计出货量已超过 100 万台，并且处于供不应求的状态。其基础款定价 299 美元，折合人民币大概是 2100 元，这其中还包含了雷朋的品牌溢价。显然，以 Ray-Ban Meta 智能眼镜为代表的 AI 智能眼镜已经得到了市场的验证。第二代 Ray-Ban Meta 智能眼镜的硬件成本大概是 1200 元，其中采用 4nm 工艺制程的主控芯片高通 AR1 Gen 1，大概占了三分之一，接近 400 元。事实上，将高通 AR1 Gen 1 芯片用于 AI 智能眼镜中，明显性能过剩了，因为 AI 智能眼镜完全不需要用到 AR1 Gen 1 芯片中的 3Dof 和双目全彩显示功能。如果将高通 AR1 Gen 1 芯片替换为采用 6nm 工艺制程的恒玄 BES2800 芯片，AI 智能眼镜的硬件成本还能大幅降低。当然，苹果发布 AI 智能眼镜，一定会采用自研的主控芯片。参考 AirPods 的定价，增加摄像头、镜框等硬件配置的 AI 智能眼镜，价格可能是 AirPods 的两倍，价格区间可能是 2000-4000 元。但是，如果大幅采用国产供应链的硬件配置，未来国内品牌基础款 AI 智能眼镜的定价区间很可能是 500-1000 元，这个价格对于大部分人来说都是可以接受的。因此在价格方面，AI 智能眼镜也已具备大规模普及的市场基础。接下来，我们重新回归到 OpenAI 与苹果可能达成的战略合作上。很明显，GPT-4o 拥有比 Llama 3 多模态大模型更强大的综合能力，而苹果也拥有非常卓越的消费级终端硬件的设计和运营能力。如果 OpenAI 和苹果达成长期合作，GPT-4o 加上苹果 AI 智能眼镜的组合，或许这才是 AI 时代最值得期待的事情。苹果 AI 智能眼镜的产品形态可能会类似于 Meta 智能眼镜，产品定位类似于 AirPods，作为配件与 iPhone 一起使用。其实我们不难想象，在 GPT-4o 官方演示的数学方程解题过程中，如果我们佩戴 AI 智能眼镜，通过实时语音与 GPT-4o 交谈，这不就完全可以替代家长辅导孩子作业了吗？当然，AI 智能眼镜的应用场景远不止此，依靠 GPT-4o 强大的视觉理解能力，它还可以帮助我们在现实生活中处理各式各样的复杂任务，比如技能培训、旅游导览、烹饪助手等等。总之，GPT-4o 已经来了，苹果的 AI 智能眼镜还会远吗？




